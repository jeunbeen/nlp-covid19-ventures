{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from konlpy.tag import Okt; okt = Okt()\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from scipy import sparse\r\n",
    "\r\n",
    "from gensim.models.word2vec import Word2Vec\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv('신문기사모음(9.17)(utf8).csv')\r\n",
    "text = data.body"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, document in enumerate(text):\r\n",
    "\r\n",
    "    # okt = Okt()\r\n",
    "    clean_words = []\r\n",
    "    for word in okt.pos(document, stem=True): #어간 추출\r\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Foreign', 'Suffix' ,'URL', 'Conjunction']: #조사, 어미, 구두점, 외국어/기호, url 제외 \r\n",
    "            clean_words.append(word[0])\r\n",
    "    # print(clean_words) \r\n",
    "    document = ' '.join(clean_words)\r\n",
    "    # print(document) \r\n",
    "    text[i] = document\r\n",
    "\r\n",
    "print(text[0], text[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenize \r\n",
    "okt=Okt()\r\n",
    "tokens = []\r\n",
    "stopwords = ['하다', '되다', '있다', '들다', '들', '기자', '오다', '돼다', '년']\r\n",
    "# stopwords = ['기자', '년']\r\n",
    "\r\n",
    "\r\n",
    "'''각 기사별로 토큰들 나눠서 리스트로 만들기'''\r\n",
    "# [ [1번 기사의 토큰들], [2번 기사의 토큰들], [3번 기사의 토큰들]... [n번 기사의 토큰들] ]\r\n",
    "for line in text:\r\n",
    "    tokens.append(okt.morphs(line))\r\n",
    "\r\n",
    "# noun -> 각 기사별 명사 토큰 list를 모아놓은 list   (list를 모은 list)\r\n",
    "\r\n",
    "\r\n",
    "# 새로운 dict tokens_processed 생성 \r\n",
    "# tokens_processed = {}\r\n",
    "whole = []\r\n",
    "\r\n",
    "\r\n",
    "# 기사 하나하나 마다 [토큰 리스트]에 담긴 한글자짜리 토큰 제거\r\n",
    "temp_index = 0\r\n",
    "for lst in tokens:\r\n",
    "    ''' 여기서 각 기사 한줄 한줄씩 처리함 '''    \r\n",
    "    \r\n",
    "\r\n",
    "    # 딕셔너리에 담을 토큰들 모아놓는 리스트....\r\n",
    "    temp_words_list = []\r\n",
    "    \r\n",
    "\r\n",
    "    for wd in lst:\r\n",
    "        ''' 여기서 기사별 [토큰list] 속 단어(토큰)들을 처리함 '''\r\n",
    "        \r\n",
    "        if len(wd) < 2:\r\n",
    "            # 한글자 짜리면 무시하고 지나침\r\n",
    "            continue\r\n",
    "            \r\n",
    "        if wd in stopwords:\r\n",
    "            # 불용어에 해당하는 단어면 무시하고 지나침\r\n",
    "            continue\r\n",
    "        \r\n",
    "        # 둘다 해당하지 않으면 임시 리스트에 담기\r\n",
    "        temp_words_list.append(wd)\r\n",
    "    \r\n",
    "    \r\n",
    "    # 필터링한 데이터 저장\r\n",
    "    # tokens_processed[temp_index] = temp_words_list\r\n",
    "    whole.append(temp_words_list)\r\n",
    "    temp_index +=1\r\n",
    "\r\n",
    "    ''' \r\n",
    "      0 : [ 기사1 토큰리스트 ]\r\n",
    "      1 : [ 기사2 토큰리스트 ]\r\n",
    "      2 : [ 기사3 토큰리스트 ]\r\n",
    "    ...\r\n",
    "    122 : [ 기사122 토큰리스트 ]\r\n",
    "    \r\n",
    "    (temp_index)    :  (temp_wors_list) \r\n",
    "    이런 형식으로 dictionary(tokens_processed)에 저장\r\n",
    "    \r\n",
    "    '''\r\n",
    "    \r\n",
    "# tokens_processed\r\n",
    "# whole[:2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 동시출현 기반 연관어 분석\r\n",
    "#### 같은 문맥에서 중복되는 단어들은 하나로 처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = {}   #동시출현 빈도가 저장될 dict\r\n",
    "for line in whole:\r\n",
    "    words = list(set(line))   #단어별로 분리한 것을 set에 넣어 중복 제거하고, 다시 list로 변경\r\n",
    "    for i, a in enumerate(words):\r\n",
    "        for b in words[i+1:]:\r\n",
    "            if a == b: continue   #같은 단어의 경우는 세지 않음\r\n",
    "            elif a > b:   #A, B와 B, A가 다르게 세어지는것을 막기 위해 항상 a < b로 순서 고정\r\n",
    "                count[b, a] = count.get((b, a), 0) + 1   #키가 이미 딕셔너리에 있는지 확인하고 키가 없으면 기본값으로 설정\r\n",
    "            else:\r\n",
    "                count[a, b] = count.get((a, b), 0) + 1   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 데이터프레임으로 예쁘게 만들기\r\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\r\n",
    "\r\n",
    "# [term1, term2, freq] 리스트 생성\r\n",
    "list1 = []\r\n",
    "for i in range(len(df)):\r\n",
    "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])\r\n",
    "\r\n",
    "df2 = pd.DataFrame(list1, columns=['term1', 'term2', 'freq'])   #헤더 만들어서 데이터프레임 생성\r\n",
    "df3 = df2.sort_values(by=['freq'], ascending=False)   #frequency 큰 순으로 정렬\r\n",
    "df3 = df3.reset_index(drop=True) #index 다시 만들기\r\n",
    "\r\n",
    "df3.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 통계적 가중치 기반 연관어 분석\r\n",
    "#### 유사도 구하기 위해 단어마다 가중치 할당\r\n",
    "- 출현 빈도, tf-idf, tf-icf, cosine similarity, jaccard similarity, overlap imilarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vec = TfidfVectorizer()\r\n",
    "vector_lines = vec.fit_transform(text) # tfidf 가중치 할당\r\n",
    "A = vector_lines.toarray()\r\n",
    "A.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "A = A.transpose() # 문서-단어 매트릭스를 단어-문서 매트릭스로 전치 (문서 간 유사도 아닌 단어 간 유사도 구하기 위해)\r\n",
    "A.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "A_sparse = sparse.csr_matrix(A)\r\n",
    "similarities_sparse = cosine_similarity(A_sparse, dense_output=False) # cosine similarity 계산\r\n",
    "list(similarities_sparse.todok().items())[35000:35010]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vec.get_feature_names()[1025:1030] # 각 단어 명칭에 접근"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 데이터프레임으로 예쁘게 만들기\r\n",
    "df = pd.DataFrame(list(similarities_sparse.todok().items()), columns=[\"words\", \"weight\"])\r\n",
    "\r\n",
    "df2 = df.sort_values(by=['weight'], ascending=False) # 단어 페어 간 유사도 높은 순으로 정렬\r\n",
    "df2 = df2.reset_index(drop=True)\r\n",
    "\r\n",
    "df3 = df2.loc[np.round(df2['weight']) < 1] # 같은 단어끼리 페어는 유사도 1, 1 미만 페어만 추출\r\n",
    "df3 = df3.reset_index(drop=True)\r\n",
    "df3.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## word2vec 기반 연관어 분석\r\n",
    "#### 2013년 구글에서 제안, 두 가지 가정 기반\r\n",
    "1. 단어의 의미는 그 단어 주변 분포로 이해될 수 있다.\r\n",
    "2. 단어의 의미는 단어 벡터 안에 인코딩 될 수 있다.\r\n",
    "\r\n",
    "#### 앞 두개와 다른 점\r\n",
    "1. 같은 문맥에서의 1) 출현 빈도 뿐 아니라 2) 단어 위치, 3) 순서에 따라서도 가중치를 다르게 준다.\r\n",
    "2. 가중치 산출 후 앞에서와 같이 유사도 공식 이용해 단어간 유사도 산출한다.\r\n",
    "\r\n",
    "#### 두 개 알고리즘\r\n",
    "1. CBOW: 주변 단어로 중심단어 예측\r\n",
    "2. Skip-gram: 중심 단어로 주변 단어 예측 (window size 따라 반복학습 더 많이 함 -> 더 정확한 경우 많다)\r\n",
    "\r\n",
    "#### 참조\r\n",
    "Distributed Representations of Words and Phrases and their Compositionality\r\n",
    "(https://arxiv.org/pdf/1310.4546.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "txt = []\r\n",
    "for line in whole:\r\n",
    "    txt.extend(list(set(line)))\r\n",
    "\r\n",
    "model = Word2Vec(txt, vector_size=100, window = 5, min_count=2, workers=4, sg=1) # skip-gram, window=주변단어, min_count=전체문서에서 최소출현횟수, workers=cpu 코어수\r\n",
    "# model.wv.similarity('게임', '규제')\r\n",
    "# model.wv.most_similar(positive=[\"게임\"], topn=5)\r\n",
    "model.wv.index_to_key"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gephi 이용\r\n",
    "\r\n",
    "- Gephi 다운로드 (https://gephi.org/)\r\n",
    "-> Java 1.8 이상 필요 (https://github.com/ojdkbuild/ojdkbuild)\r\n",
    "\r\n",
    "- 또는 yEd graph editor 다운로드"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len((np.where(df3_pos['freq']>=25))[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Centrality\r\n",
    "\r\n",
    "G=nx.Graph()\r\n",
    "for i in range(1071):\r\n",
    "    #print(pair)\r\n",
    "    \r\n",
    "    G.add_edge(df3_pos['term1'][i], df3_pos['term2'][i], weight=int(df3_pos['freq'][i]))\r\n",
    "# Compute centralities for nodes.\r\n",
    "# The degree centrality values are normalized by dividing by the maximum possible degree in a simple graph n-1 where n is the number of nodes in G.\r\n",
    "dgr = nx.degree_centrality(G)\r\n",
    "btw = nx.betweenness_centrality(G)\r\n",
    "cls = nx.closeness_centrality(G)\r\n",
    "# itemgetter(0): key 또는 itemgetter(1): value로 sort key, reverse=True (descending order)\r\n",
    "sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\r\n",
    "sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\r\n",
    "sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)\r\n",
    "print(\"** degree **\")\r\n",
    "for x in range(20):\r\n",
    "    print(sorted_dgr[x])\r\n",
    "print(\"** betweenness **\")\r\n",
    "for x in range(20):\r\n",
    "    print(sorted_btw[x])\r\n",
    "print(\"** closeness **\")\r\n",
    "for x in range(20):\r\n",
    "    print(sorted_cls[x])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#단어끼리 서로 빈도를 세는 데이터셋을 만들었을 때 Gaphi로 시각화하는 것 전단계: graphml 확장자 형식으로 만들기\r\n",
    "class MakeGraphml:\r\n",
    "    def make_graphml(self, pair_file, graphml_file):\r\n",
    "        out = open(graphml_file, 'w', encoding = 'utf-8')\r\n",
    "        entity = []\r\n",
    "        e_dict = {}\r\n",
    "        count = []\r\n",
    "        for i in range(len(pair_file)):\r\n",
    "            e1 = pair_file.iloc[i,0]\r\n",
    "            e2 = pair_file.iloc[i,1]\r\n",
    "            #frq = ((word_dict[e1], word_dict[e2]),  pair.split('\\t')[2])\r\n",
    "            frq = ((e1, e2), pair_file.iloc[i,2])\r\n",
    "            if frq not in count: count.append(frq)   # ((a, b), frq)\r\n",
    "            if e1 not in entity: entity.append(e1)\r\n",
    "            if e2 not in entity: entity.append(e2)\r\n",
    "        print('# terms: %s'% len(entity))\r\n",
    "        #create e_dict {entity: id} from entity\r\n",
    "        for i, w in enumerate(entity):\r\n",
    "            e_dict[w] = i + 1 # {word: id}\r\n",
    "        out.write(\r\n",
    "            \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><graphml xmlns=\\\"http://graphml.graphdrawing.org/xmlns\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://graphml.graphdrawing.org/xmlnshttp://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\\\">\" +\r\n",
    "            \"<key id=\\\"d1\\\" for=\\\"edge\\\" attr.name=\\\"weight\\\" attr.type=\\\"double\\\"/>\" +\r\n",
    "            \"<key id=\\\"d0\\\" for=\\\"node\\\" attr.name=\\\"label\\\" attr.type=\\\"string\\\"/>\" +\r\n",
    "            \"<graph id=\\\"Entity\\\" edgedefault=\\\"undirected\\\">\" + \"\\n\")\r\n",
    "        # nodes\r\n",
    "        for i in entity:\r\n",
    "            out.write(\"<node id=\\\"\" + str(e_dict[i]) +\"\\\">\" + \"\\n\")\r\n",
    "            out.write(\"<data key=\\\"d0\\\">\" + i + \"</data>\" + \"\\n\")\r\n",
    "            out.write(\"</node>\")\r\n",
    "        # edges\r\n",
    "        for y in range(len(count)):\r\n",
    "            out.write(\"<edge source=\\\"\" + str(e_dict[count[y][0][0]]) + \"\\\" target=\\\"\" + str(e_dict[count[y][0][1]]) + \"\\\">\" + \"\\n\")\r\n",
    "            out.write(\"<data key=\\\"d1\\\">\" + str(count[y][1]) + \"</data>\" + \"\\n\")\r\n",
    "            #out.write(\"<edge source=\\\"\" + str(count[y][0][0]) + \"\\\" target=\\\"\" + str(count[y][0][1]) +\"\\\">\"+\"\\n\")\r\n",
    "            #out.write(\"<data key=\\\"d1\\\">\" + str(count[y][1]) +\"</data>\"+\"\\n\")\r\n",
    "            out.write(\"</edge>\")\r\n",
    "        out.write(\"</graph> </graphml>\") \r\n",
    "        print('now you can see %s' % graphml_file)\r\n",
    "        #pairs.close()\r\n",
    "        out.close()\r\n",
    "gm = MakeGraphml()\r\n",
    "graphml_file = '파일명.graphml'\r\n",
    "#iloc는 인덱스 index of location 열에서 : 써야 함 (열 전체 보여주려면)\r\n",
    "gm.make_graphml(df3.iloc[0:1027,:], graphml_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Visualization\r\n",
    "\r\n",
    "# class MakeGraphml:\r\n",
    "#     def make_graphml(self, pair_file, graphml_file):\r\n",
    "#         out = open(graphml_file, 'w', encoding = 'utf-8')\r\n",
    "\r\n",
    "#         entity = []\r\n",
    "#         e_dict = {}\r\n",
    "#         count = []\r\n",
    "#         for i in range(len(pair_file)):\r\n",
    "#             e1 = pair_file.iloc[i, 0]\r\n",
    "#             e2 = pair_file.iloc[i, 1]\r\n",
    "#             # frq = ((word_dict[e1], word_dict[e2]), pair,split('\\t')[2])\r\n",
    "#             frq = ((e1, e2), pair_file.iloc[i, 2])\r\n",
    "#             if frq not in count: count.append(frq)   # ((a, b), frq)\r\n",
    "#             if e1 not in entity: entity.append(e1)\r\n",
    "#             if e2 not in entity: entity.append(e2)\r\n",
    "#         print('# terms: %s'% len(entity))\r\n",
    "\r\n",
    "#         for i, w in enumerate(entity):\r\n",
    "#             e_dict[w] = i + 1   # {word: id}\r\n",
    "\r\n",
    "#         out.write(\r\n",
    "#             \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><graphml xmlns=\\\"http://graphml.graphdrawing.org/xmlns\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:schemaLocation=\\\"http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\\\">\" \r\n",
    "#             +\r\n",
    "#             \"<key id=\\\"d1\\\" for=\\\"edge\\\" attr.name=\\\"weight\\\" attr.type=\\\"double\\\"/>\"\r\n",
    "#             +\r\n",
    "#             \"<key id=\\\"d0\\\" for=\\\"node\\\" attr.name=\\\"label\\\" attr.type=\\\"string\\\"/>\"\r\n",
    "#             +\r\n",
    "#             \"graph id=\\\"Entity\\\" edgedefault=\\\"undirected\\\">\" + \"\\n\")\r\n",
    "\r\n",
    "#         # nodes\r\n",
    "#         for i in entity:\r\n",
    "#             out.write(\"<node id=\\\"\" + str(e_dict[i]) + \"\\\">\" + \"\\n\")\r\n",
    "#             out.write(\"<data key=\\\"d0\\\">\" + i + \"</data>\" + \"\\n\")\r\n",
    "#             out.write(\"</node>\")\r\n",
    "\r\n",
    "#         # edges\r\n",
    "#         for y in range(len(count)):\r\n",
    "#             out.write(\"<edge source=\\\"\" + str(e_dict[count[y][0][0]]) + \"\\\" target=\\\"\" + str(e_dict[count[y][0][1]]) + \"\\\">\" + \"\\n\")\r\n",
    "#             out.write(\"<data key=\\\"d1\\\">\" + str(count[y][1]) + \"</data>\" + \"\\n\")\r\n",
    "#             out.write(\"</edge>\")\r\n",
    "\r\n",
    "#         out.write(\"</graph> </graphml>\")\r\n",
    "#         print('now you can see %s' % graphml_file)\r\n",
    "\r\n",
    "#         out.close"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# gm = MakeGraphml()\r\n",
    "\r\n",
    "# # 파일경로와 파일명 입력 (여기서는 파일명만)\r\n",
    "# graphml_file = '연관어네트워크(10).graphml'\r\n",
    "\r\n",
    "# # 연관도(동시출현 빈도)가 10 초과인 행들만 선정\r\n",
    "# gm.make_graphml(df3_pos.iloc[0: (len(np.where(df3_pos['freq'] > 10)[0])), :], graphml_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NetworkX 패키지 이용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "중심성이란 그래프 이론에서 쓰이는 용어입니다. 하지만 단어 간의 연관도를 링크로 표현하면 하나의 그래프가 형성되기 때문에 연관성 분석에서 다루겠습니다. 앞에서 단어 페어 간의 연관도를 계산하는 방법들을 배웠습니다. 하지만 전체 단어군에서 개별 단어(노드)의 상대적 중요성을 알 수는 없습니다.\r\n",
    "\r\n",
    "예를 들어 A와 B라는 단어 간의 연관도를 구했지만, 전체 단어 중에서 A라는 단어가 상대적으로 지니는 중요성은 알지 못합니다. 또한, 그 중요성을 어떠한 잣대로 살펴봐야 할지도 여러 가지가 있습니다. 하지만 각 단어별 중심성 계수를 구하면 단어별 상대적 중요성을 중심성 계수 성격별로 구할 수 있습니다. 그 후 이 결과를 이용해 리포트할 수도 있고 시각화 할 때 노드의 크기, 색상, 진하기를 달리할 수도 있습니다. 대표적인 중심성 계수로는 연결 중심성(degree centrality), 매개 중심성(betweenness centrality), 고유벡터 중심성(eigenvector centrality) 등이 있습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 연결 중심성: 하나의 노드가 직접적으로 몇 개의 노드와 연결돼 있는지 측정합니다. 즉, 하나의 단어와 직접적으로 연관도 링크를 지닌 단어의 개수를 측정합니다. 단순히 1개의 링크 거리만을 고려함으로 국지적인 범위에서만의 역할을 보게 됩니다.\r\n",
    "\r\n",
    "- 근접 중심성: 단순히 1개의 링크 거리만을 고려하는 것은 전체 네트워크상에서의 노드의 영향력을 파악하기 어렵습니다. 따라서 직접적인 연결(1개 링크 거리)뿐만 아니라 간접적인 연결(2개 이상의 링크 거리)까지 포함해서 중심성을 측정합니다. 즉, 특정 단어와 연속적인 링크로 연결되는 모든 단어와의 거리에 따른 평균적인 연관도를 측정하여 글로벌적인 중요성 판단이 가능해집니다.\r\n",
    "\r\n",
    "- 매개 중심성: 해당 노드가 중계자 또는 브로커 역할을 얼마나 잘하는지 측정합니다. 즉, 노드 간 링크를 타고 건너갈 때 핵심적으로 통과해야만 하는 노드를 찾을 때 용이합니다. 매개 중심성이 크다면 네트워크 내의 의사소통의 흐름에 영향을 줄 소지가 많습니다. 매개 중심성은 작업자 간 워크플로우를 분석할 때 매우 용이하지만 텍스트 마이닝에서는 자주 활용되지는 않습니다.\r\n",
    "\r\n",
    "- 고유벡터 중심성: 각 노드마다 중요성을 부과할 때 해당 노드와 연결된 노드들의 중심성을 고려합니다. 즉, 높은 고유벡터 중심성을 가진 노드는 높은 점수를 가진 많은 노드와 연결되었음을 의미합니다. 고유벡터 중심성을 변형한 알고리즘 중 구글 검색 알고리즘 PageRank가 유명합니다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 중심성 척도 계산 위한 그래프 생성\r\n",
    "G_pos = nx.Graph()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 빈도수 10 이상인 단어쌍에 대해서만 edge(간선) 표현\r\n",
    "for i in range((len(np.where(df3_pos['freq'] > 10)[0]))):\r\n",
    "    G_pos.add_edge(df3_pos['term1'][i], df3_pos['term2'][i], weigh=int(df3_pos['freq'][i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dgr = nx.degree_centrality(G_pos)   # 연결 중심성\r\n",
    "btw = nx.betweenness_centrality(G_pos)   # 매개 중심성\r\n",
    "cls = nx.closeness_centrality(G_pos)   # 근접 중심성\r\n",
    "egv = nx.eigenvector_centrality(G_pos)   # 고유벡터 중심성\r\n",
    "pgr = nx.pagerank(G_pos)   # 페이지랭크"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 중심성 큰 순서로 정렬\r\n",
    "sorted_dgr = sorted(dgr.items(), key = operator.itemgetter(1), reverse = True)\r\n",
    "sorted_btw = sorted(btw.items(), key = operator.itemgetter(1), reverse = True)\r\n",
    "sorted_cls = sorted(cls.items(), key = operator.itemgetter(1), reverse = True)\r\n",
    "sorted_egv = sorted(egv.items(), key = operator.itemgetter(1), reverse = True)\r\n",
    "sorted_pgr = sorted(pgr.items(), key = operator.itemgetter(1), reverse = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 단어 네트워크 그릴 그래프 생성\r\n",
    "G = nx.Graph()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 페이지 랭크에 따라 두 노드 사이 연관성 결정 (단어쌍의 연관성)\r\n",
    "# 연결 중심성으로 계산한 척도에 따라 노드 크기 결정 (단어의 등장 빈도수)\r\n",
    "\r\n",
    "for i in range(len(sorted_pgr)):\r\n",
    "    G.add_node(sorted_pgr[i][0], nodesize = sorted_dgr[i][1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range((len(np.where(df3_pos['freq'] > 10)[0]))):\r\n",
    "    G.add_weighted_edges_from([(df3_pos['term1'][i], df3_pos['term2'][i], int(df3_pos['freq'][i]))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 노드 크기 조정\r\n",
    "sizes = [G.nodes[node]['nodesize'] * 5000 for node in G]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "options = {\r\n",
    "    'edge_color': '#a2a1b3',\r\n",
    "    'width': 1,\r\n",
    "    'with_labels': True,\r\n",
    "    'font_weight': 'regular',\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.font_manager as fm\r\n",
    "from matplotlib import rc\r\n",
    "font_name = fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\r\n",
    "rc('font', family=font_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\r\n",
    "\r\n",
    "# font_family = 'KoPubDotum' -> 위 셀 지우고 이것만 추가해도 돌아감\r\n",
    "nx.draw(G, font_family = font_name, font_size = 16,\r\n",
    "        node_size = sizes, node_color = list(pgr.values()), cmap=plt.cm.Pastel1,   # pgr weight에 따라 노드 색상 다르게\r\n",
    "        pos = nx.spring_layout(G, k = 3.5, iterations = 100), **options)\r\n",
    "        \r\n",
    "ax = plt.gca()\r\n",
    "# ax.collections[0].set_edgecolor(\"#0f03ff\")   # 노드 테두리 색\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "3384638f5810346ec4e47c085d16c91a672dec4d6598dd6288d516a5e6f3bebf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}