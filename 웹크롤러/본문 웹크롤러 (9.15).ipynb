{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e9c14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# install pakages  (  -> first time )\n",
    "\n",
    "'''\n",
    "!pip install urllib3\n",
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "'''\n",
    "\n",
    "\n",
    "# 기본 세팅\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote, urlencode, quote_plus, unquote\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import datetime as dt\n",
    "import time as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ssl\n",
    "from ssl import SSLError\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbfa48",
   "metadata": {},
   "source": [
    "# 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d0a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n",
      "자료가 없습니다.\n",
      "작업 완료!\n",
      "작업 소요시간 : 약 0.0분\n",
      "기사 개수 :  0\n",
      "작업 완료 시간 :  2021-09-15 15:05:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('www.chosun.com')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 조선일보 외 타 신문사 제외\n",
    "        if 'www.chosun.com' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "                \n",
    "            page_tags = page.find_all()\n",
    "            body = eval(str(page_tags[0]).split('globalContent=')[1].split('\"created_date\"')[0].split('\"content:\"')[0]+'}')\n",
    "            body[\"content_elements\"]\n",
    "            temp = []\n",
    "            for element in body[\"content_elements\"]:\n",
    "                try:\n",
    "                    temp.append(element['content'])\n",
    "                except:\n",
    "                    pass\n",
    "            body = '\\n'.join(temp)\n",
    "\n",
    "            \n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '조선일보'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('조선일보 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca9ee9",
   "metadata": {},
   "source": [
    "# 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e6a218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n",
      "자료가 없습니다.\n",
      "작업 완료!\n",
      "작업 소요시간 : 약 0.0분\n",
      "기사 개수 :  0\n",
      "작업 완료 시간 :  2021-09-15 15:05:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('joongang.co.kr')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 중앙일보 외 타 신문사 제외\n",
    "        if 'www.joongang.co.kr' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "            body = page.find_all('div', 'article_body')[0].text\n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '중앙일보'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('중앙일보 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076dac4",
   "metadata": {},
   "source": [
    "# 동아일보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdc8fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.04분\n",
      "기사 개수 :  4\n",
      "작업 완료 시간 :  2021-09-15 15:05:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('donga')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 동아일보 외 타 신문사 제외\n",
    "        if 'www.donga.com' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "   \n",
    "            # body type 1\n",
    "            target = page.find_all('div', 'article_view')[0].find_all('div')\n",
    "            target = target[0]\n",
    "            temp = []\n",
    "            for element in target:\n",
    "                if element.name == None:\n",
    "                    temp.append(element)\n",
    "\n",
    "            body = ' '.join(temp).lstrip().rstrip()                \n",
    "\n",
    "            \n",
    "            # body type 2\n",
    "            if len(body)<=200:\n",
    "                target = page.find_all('div', 'article_view')[0].find_all('div')\n",
    "                target = target[0]\n",
    "                temp = []\n",
    "                target\n",
    "                for element in target:\n",
    "                    if element.name == 'div':\n",
    "                        for element2 in element:\n",
    "                            if element2.name == None:\n",
    "                                temp.append(element2)\n",
    "\n",
    "                body = ' '.join(temp)\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '동아일보'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "                \n",
    "  \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        #dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        #dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        #dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        #dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('동아일보 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11249f3b",
   "metadata": {},
   "source": [
    "# 한겨레"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0382861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.02분\n",
      "기사 개수 :  1\n",
      "작업 완료 시간 :  2021-09-15 15:05:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('hani.co.kr')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 한겨레 외 타 신문사 제외\n",
    "        if 'www.hani.co.kr' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "            body = page.find_all('div', 'article-text')[0].text.rsplit('@hani.co.kr', 1)[0]\n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '한겨레'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('한겨레 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbae96",
   "metadata": {},
   "source": [
    "# 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863d8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.06분\n",
      "기사 개수 :  6\n",
      "작업 완료 시간 :  2021-09-15 15:05:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('khan.co.kr')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 경향 외 타 신문사 제외\n",
    "        if 'khan.co.kr' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "\n",
    "            # body = page.find_all('div', 'art_body')[0].text\n",
    "            body = \"\"\n",
    "            for line in page.find_all('p', class_='content_text'):\n",
    "                body += line.get_text()\n",
    "            title = page.find_all('title')[0].text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '경향'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('경향 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491794c4",
   "metadata": {},
   "source": [
    "# 매일경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c86b1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [00:14<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.24분\n",
      "기사 개수 :  20\n",
      "작업 완료 시간 :  2021-09-15 15:05:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('mk.co.kr')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 매일경제 외 타 신문사 제외\n",
    "        if 'game' in urls[idx] in urls[idx]:\n",
    "            continue\n",
    "            \n",
    "        elif 'news.mk.co.kr' in urls[idx]:\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "\n",
    "            try:    \n",
    "                target = page.find_all('div', id='artText')[0]\n",
    "\n",
    "                temp = []\n",
    "                for element in target:\n",
    "                    if element.name == None:\n",
    "                        if ('=' not in element) & ('div' not in element):\n",
    "                            if element.lstrip().rstrip() != '':\n",
    "                                if '//' not in element:\n",
    "                                    temp.append(element.lstrip().rstrip())\n",
    "                body = ' '.join(temp)\n",
    "            except:\n",
    "                body = page.find_all('div', 'art_txt')[0].text.lstrip().rstrip()\n",
    "                \n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '매일경제'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('매일경제 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920a186",
   "metadata": {},
   "source": [
    "# 한국경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ee93b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:46<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.77분\n",
      "기사 개수 :  26\n",
      "작업 완료 시간 :  2021-09-15 15:06:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('hankyung')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 한국경제 외 타 신문사 제외\n",
    "        if 'www.hankyung.com' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "            body = page.find('div', id='articletxt').text\n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '한국경제'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('한국경제 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4fbb8",
   "metadata": {},
   "source": [
    "# 디지털타임스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51ed619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:06<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.12분\n",
      "기사 개수 :  15\n",
      "작업 완료 시간 :  2021-09-15 15:06:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('dt.co.kr')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 디지털타임스 외 타 신문사 제외\n",
    "        if 'www.dt.co.kr' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "            body = page.find_all('div','art_txt')[0].text.lstrip().rstrip()\n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '디지털타임스'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('디지털타임스 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880a1f6",
   "metadata": {},
   "source": [
    "# 전자신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88f680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 82/82 [00:25<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.43분\n",
      "기사 개수 :  50\n",
      "작업 완료 시간 :  2021-09-15 15:06:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time  = t.time()\n",
    "\n",
    "for is_data in range(1):\n",
    "    \n",
    "    # url 가져오기 (csv파일)\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\n",
    "    \n",
    "    if len(url_data)==0:\n",
    "        print('기사가 없습니다.')\n",
    "        continue\n",
    "    else:\n",
    "        url_data.columns = ['date', 'urls']\n",
    "\n",
    "        # 특정 url만 필터링\n",
    "        url_data = url_data[ url_data['urls'].str.contains('etnews')]\n",
    "\n",
    "        url_data = url_data.reset_index()\n",
    "        url_data.columns = ['index', 'date', 'urls']\n",
    "        urls = list(url_data['urls'])\n",
    "\n",
    "    # 데이터가 들어갈 공간\n",
    "    data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "    #url 접속 후 크롤링\n",
    "    print(\"### 본문 수집 ###\")\n",
    "    for idx in tqdm(range(len(urls))):\n",
    "\n",
    "        # 초기화\n",
    "        body = ''\n",
    "        title = ''\n",
    "        day = ''\n",
    "        company = ''\n",
    "\n",
    "\n",
    "        # 전자신문 외 타 신문사 제외\n",
    "        if 'www.etnews.com' in urls[idx]:\n",
    "\n",
    "\n",
    "            ### parsing ###\n",
    "            try:\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')  \n",
    "\n",
    "            except HTTPError:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\n",
    "                t.sleep(20)\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "                )\n",
    "                t.sleep(0.3)\n",
    "                page = req.content\n",
    "                page = bs(page, 'html.parser')   \n",
    "                \n",
    "            body = page.body.p.text\n",
    "            title = page.title.text\n",
    "            day = url_data['date'][idx]\n",
    "            company = '전자신문'\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    '''\n",
    "    데이터 내보내기 (뉴스본문)\n",
    "    '''\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\n",
    "    if len(dataframe) ==0:\n",
    "        print('자료가 없습니다.')\n",
    "        pass\n",
    "    else:\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "        # 본문 내 특수문자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "        # 한자 제거\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "    dataframe.to_csv('전자신문 크롤링.csv', encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('기사 개수 : ', len(dataframe))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3384638f5810346ec4e47c085d16c91a672dec4d6598dd6288d516a5e6f3bebf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
